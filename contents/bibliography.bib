@book{2017c,
  title = {The {{Age}} of {{Surveillance Capitalism}}: {{The Fight}} for a {{Human Future}} at the {{New Frontier}} of {{Power}}},
  shorttitle = {The Age of Surveillance Capitalism},
  author = {Zuboff, Shoshana},
  date = {2017-06-27T21:33:47+00:00},
  edition = {First trade paperback edition},
  publisher = {PublicAffairs},
  location = {New York, NY},
  url = {https://www.hachettebookgroup.com/titles/shoshana-zuboff/the-age-of-surveillance-capitalism/9781610395694/?lens=publicaffairs},
  abstract = {"Shoshana Zuboff, named "the true prophet of the information age" by the Financial Times, has always been ahead of her time. Her seminal book In the Age of the Smart Machine foresaw the consequences of a then-unfolding era of computer technology. Now, three decades later she asks why the once-celebrated miracle of digital is turning into a nightmare. Zuboff tackles the social, political, business, personal, and technological meaning of "surveillance capitalism" as an unprecedented new market form. It is not simply about tracking us and selling ads, it is the business model for an ominous new marketplace that aims at nothing less than predicting and modifying our everyday behavior--where we go, what we do, what we say, how we feel, who we're with. The consequences of surveillance capitalism for us as individuals and as a society vividly come to life in The Age of Surveillance Capitalism's pathbreaking analysis of power. The threat has shifted from a totalitarian "big brother" state to a universal global architecture of automatic sensors and smart capabilities: A "big other" that imposes a fundamentally new form of power and unprecedented concentrations of knowledge in private companies--free from democratic oversight and control"--},
  isbn = {978-1-61039-569-4 978-1-5417-5800-1},
  langid = {american},
  pagetotal = {691},
  file = {/Users/maehr0000/Zotero/storage/PYYHNLYP/Shoshana Zuboff - The Age of Surveillance Capitalism_ The Fight for a Human Future at the New Frontier of Power (2019, PublicAffairs) - libgen.li.epub;/Users/maehr0000/Zotero/storage/TSUS5UZB/Zuboff - 2020 - The age of surveillance capitalism the fight for a human future at the new frontier of power.pdf;/Users/maehr0000/Zotero/storage/S2NAG237/9781610395694.html}
}

@online{2018e,
  title = {Jobs für Flüchtlinge - Algorithmus verteilt neu Asylbewerber auf Kantone},
  date = {2018-05-10T08:57:00+02:00},
  url = {https://www.srf.ch/news/schweiz/jobs-fuer-fluechtlinge-algorithmus-verteilt-neu-asylbewerber-auf-kantone},
  urldate = {2024-11-25},
  abstract = {Für die «Big Data» des Migrationsamtes interessieren sich weltweit auch andere Staaten.},
  langid = {ngerman},
  organization = {Schweizer Radio und Fernsehen (SRF)},
  file = {/Users/maehr0000/Zotero/storage/77I7JNJQ/jobs-fuer-fluechtlinge-algorithmus-verteilt-neu-asylbewerber-auf-kantone.html}
}

@online{2018f,
  title = {Algorithmus verbessert Erwerbschancen von Flüchtlingen},
  date = {2018-01-18},
  url = {https://ethz.ch/de/news-und-veranstaltungen/eth-news/news/2018/01/algorithmus-verbessert-erwerbschancen-von-fluechtlingen.html},
  urldate = {2024-11-25},
  abstract = {Die Erwerbstätigkeit von Asylsuchenden in der Schweiz könnte mit einem datengestützten Ansatz von 15 auf 26 Prozent erhöht werden. Zu diesem Ergebnis kommen Sozialwissenschaftler und Sozialwissenschaftlerinnen aus der Schweiz und den USA mit Beteiligung der ETH-Professur für Politikanalyse in der Wissenschaftszeitschrift «Science».},
  langid = {ngerman},
  organization = {ETH Zürich},
  file = {/Users/maehr0000/Zotero/storage/VQFSHXH6/algorithmus-verbessert-erwerbschancen-von-fluechtlingen.html}
}

@inreference{2022j,
  title = {\mkbibemph{Algorithms of }{{\mkbibemph{Oppression}}}},
  booktitle = {Wikipedia},
  date = {2022-06-28T00:04:29Z},
  url = {https://en.wikipedia.org/w/index.php?title=Algorithms_of_Oppression&oldid=1095369660},
  urldate = {2024-11-23},
  abstract = {Algorithms of Oppression: How Search Engines Reinforce Racism is a 2018 book by Safiya Umoja Noble in the fields of information science, machine learning, and human-computer interaction.},
  langid = {english},
  annotation = {Page Version ID: 1095369660},
  file = {/Users/maehr0000/Zotero/storage/NSZMZ7HF/Algorithms_of_Oppression.html}
}

@inreference{2024c,
  title = {Surveillance Capitalism},
  booktitle = {Wikipedia},
  date = {2024-08-12T09:16:35Z},
  url = {https://en.wikipedia.org/w/index.php?title=Surveillance_capitalism&oldid=1239902991},
  urldate = {2024-11-21},
  abstract = {Surveillance capitalism is a concept in political economics which denotes the widespread collection and commodification of personal data by corporations. This phenomenon is distinct from government surveillance, although the two can be mutually reinforcing. The concept of surveillance capitalism, as described by Shoshana Zuboff, is driven by a profit-making incentive, and arose as advertising companies, led by Google's AdWords, saw the possibilities of using personal data to target consumers more precisely. Increased data collection may have various benefits for individuals and society, such as self-optimization (the quantified self), societal optimizations (e.g., by smart cities) and optimized services (including various web applications). However, as capitalism focuses on expanding the proportion of social life that is open to data collection and data processing, this can have significant implications for vulnerability and control of society, as well as for privacy. The economic pressures of capitalism are driving the intensification of online connection and monitoring, with spaces of social life opening up to saturation by corporate actors, directed at making profits and/or regulating behavior. Therefore, personal data points increased in value after the possibilities of targeted advertising were known. As a result, the increasing price of data has limited access to the purchase of personal data points to the richest in society.},
  langid = {english},
  annotation = {Page Version ID: 1239902991}
}

@article{almaaitah2020,
  title = {Opportunities and Challenges in Enhancing Access to Metadata of Cultural Heritage Collections: A Survey},
  shorttitle = {Opportunities and Challenges in Enhancing Access to Metadata of Cultural Heritage Collections},
  author = {Alma’aitah, Wafa’ Za’al and Talib, Abdullah Zawawi and Osman, Mohd Azam},
  date = {2020-06-01},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {53},
  number = {5},
  pages = {3621--3646},
  issn = {1573-7462},
  doi = {10.1007/s10462-019-09773-w},
  url = {https://doi.org/10.1007/s10462-019-09773-w},
  urldate = {2025-03-03},
  abstract = {Machine processable data that narrate digital/non-digital resources are termed as metadata. Different metadata standards exist for describing various types of digital objects. Several researches have reported on how to address issues related to accessing of metadata resources. Most studies on metadata involve cultural heritage domain, and this is an indication of the importance of this domain in metadata research and development. Research on metadata in cultural heritage mainly revolves around three fundamental issues: (1) lack of quality in metadata contents in most of the cases, (2) difficulty in accessing metadata contents due largely to limited user’s knowledge on the content of the metadata, and (3) heterogeneity of the data at the level of schemas which makes the access even more difficult. The lack of quality in metadata makes it difficult for the users to retrieve and explore information that satisfies their needs. So, in order to make its contents more accessible, enhancing the metadata content is required, especially for cultural heritage collections which consist of digital objects (structured documents) described by a variety of metadata schemas. This paper presents issues and challenges in enhancing access to metadata by reviewing the existing approaches in metadata environment with a particular emphasis on cultural heritage collections. In this paper, firstly, we look at the classification of metadata which is divided into two categories namely data retrieval and information retrieval. Then, we present the analysis, findings and suggestions on how to address issues in enhancing access to metadata contents especially in cultural heritage collections. A detailed comparison is given between information retrieval and data retrieval, and it focuses on the applicability of one approach over the other. A framework that aims to improve the effectiveness of retrieval when searching metadata is also proposed and tested. The proposed framework consists of approaches and methods that are expected to enhance access to metadata especially in cultural heritage collections and be useful for those with limited knowledge on cultural heritage. The experiments were conducted on CHiC2013 which is a collection on cultural heritage. The results show a considerable enhancement over other IR approaches that use the expansion methods.},
  langid = {english},
  keywords = {Artificial Intelligence,Cultural heritage,Data retrieval,Information retrieval,Metadata},
  file = {/Users/maehr0000/Zotero/storage/C5IDBGKD/Alma’aitah et al. - 2020 - Opportunities and challenges in enhancing access to metadata of cultural heritage collections a sur.pdf}
}

@article{bansak2018,
  title = {Improving Refugee Integration through Data-Driven Algorithmic Assignment},
  author = {Bansak, Kirk and Ferwerda, Jeremy and Hainmueller, Jens and Dillon, Andrea and Hangartner, Dominik and Lawrence, Duncan and Weinstein, Jeremy},
  date = {2018-01-19},
  journaltitle = {Science},
  volume = {359},
  number = {6373},
  pages = {325--329},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aao4408},
  url = {https://www.science.org/doi/10.1126/science.aao4408},
  urldate = {2024-11-25},
  abstract = {Developed democracies are settling an increased number of refugees, many of whom face challenges integrating into host societies. We developed a flexible data-driven algorithm that assigns refugees across resettlement locations to improve integration outcomes. The algorithm uses a combination of supervised machine learning and optimal matching to discover and leverage synergies between refugee characteristics and resettlement sites. The algorithm was tested on historical registry data from two countries with different assignment regimes and refugee populations, the United States and Switzerland. Our approach led to gains of roughly 40 to 70\%, on average, in refugees’ employment outcomes relative to current assignment practices. This approach can provide governments with a practical and cost-efficient policy tool that can be immediately implemented within existing institutional structures.}
}

@inproceedings{bender2021,
  title = {On the {{Dangers}} of {{Stochastic Parrots}}: {{Can Language Models Be Too Big}}? 🦜},
  shorttitle = {On the {{Dangers}} of {{Stochastic Parrots}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  date = {2021-03-01},
  series = {{{FAccT}} '21},
  pages = {610--623},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3442188.3445922},
  url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
  urldate = {2024-11-24},
  abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  isbn = {978-1-4503-8309-7},
  file = {/Users/maehr0000/Zotero/storage/I67QAH43/Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language Models Be Too Big .pdf}
}

@inproceedings{buolamwini2018,
  title = {Gender {{Shades}}: {{Intersectional Accuracy Disparities}} in {{Commercial Gender Classification}}},
  shorttitle = {Gender {{Shades}}},
  booktitle = {Proceedings of the 1st {{Conference}} on {{Fairness}}, {{Accountability}} and {{Transparency}}},
  author = {Buolamwini, Joy and Gebru, Timnit},
  date = {2018-01-21},
  pages = {77--91},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
  urldate = {2024-11-25},
  abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
  eventtitle = {Conference on {{Fairness}}, {{Accountability}} and {{Transparency}}},
  langid = {english},
  file = {/Users/maehr0000/Zotero/storage/9VEVQA9X/Buolamwini and Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities in Commercial Gender Classification.pdf;/Users/maehr0000/Zotero/storage/F3RQCG3H/Buolamwini and Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities in Commercial Gender Classification.pdf}
}

@book{buolamwini2023,
  title = {Unmasking {{AI}}: A Story of Hope and Justice in a World of Machines},
  shorttitle = {Unmasking {{AI}}},
  author = {Buolamwini, Joy},
  date = {2023},
  edition = {First edition},
  publisher = {Random House},
  location = {New York},
  abstract = {"Dr. Joy Buolamwini is the self-described "Poet of Code" who has had a lifelong passion for computer science, engineering, and art-disciplines that, she felt, pushed the boundaries of reality. After tinkering with robotics as a high school student in Tennessee, to developing mobile apps in Zambia as a Fulbright fellow, Buolamwini eventually found herself at MIT. As a graduate student at the "Future Factory," Buolamwini's groundbreaking research revealed that AI systems-from leading tech companies-were consistently failing on non-male, non-white bodies. In Unmasking AI, Buolamwini goes beyond the news headlines about racism, colorism, and sexism in Big Tech to tell the remarkable story of how she uncovered what she calls "the coded gaze"-evidence of racial and gender bias in tech-and galvanized the movement to prevent AI harms by founding the Algorithmic Justice League. Applying an intersectional lens to both tech industry and research sector, Buolamwini shows how race, gender, and ability bias can overlap and render broad swaths of humanity vulnerable in our AI-dependent world. Computers, she reminds us, are reflections of both the aspirations and the limitations of the people who create them"--},
  isbn = {978-0-593-24185-1},
  pagetotal = {1},
  keywords = {Artificial intelligence,Discrimination in science,Moral and ethical aspects,Philosophy,Sex discrimination in science,Social aspects}
}

@article{chen2023,
  title = {Ethics and Discrimination in Artificial Intelligence-Enabled Recruitment Practices},
  author = {Chen, Zhisheng},
  date = {2023-09-13},
  journaltitle = {Humanities and Social Sciences Communications},
  shortjournal = {Humanit Soc Sci Commun},
  volume = {10},
  number = {1},
  pages = {1--12},
  publisher = {Palgrave},
  issn = {2662-9992},
  doi = {10.1057/s41599-023-02079-x},
  url = {https://www.nature.com/articles/s41599-023-02079-x},
  urldate = {2024-11-25},
  abstract = {This study aims to address the research gap on algorithmic discrimination caused by AI-enabled recruitment and explore technical and managerial solutions. The primary research approach used is a literature review. The findings suggest that AI-enabled recruitment has the potential to enhance recruitment quality, increase efficiency, and reduce transactional work. However, algorithmic bias results in discriminatory hiring practices based on gender, race, color, and personality traits. The study indicates that algorithmic bias stems from limited raw data sets and biased algorithm designers. To mitigate this issue, it is recommended to implement technical measures, such as unbiased dataset frameworks and improved algorithmic transparency, as well as management measures like internal corporate ethical governance and external oversight. Employing Grounded Theory, the study conducted survey analysis to collect firsthand data on respondents’ experiences and perceptions of AI-driven recruitment applications and discrimination.},
  langid = {english},
  keywords = {Business and management,Science,technology and society},
  file = {/Users/maehr0000/Zotero/storage/4ATMGF7N/Chen - 2023 - Ethics and discrimination in artificial intelligence-enabled recruitment practices.pdf}
}

@article{chiang2023,
  entrysubtype = {magazine},
  title = {{{ChatGPT Is}} a {{Blurry JPEG}} of the {{Web}}},
  author = {Chiang, Ted},
  date = {2023-02-09},
  journaltitle = {The New Yorker},
  issn = {0028-792X},
  url = {https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web},
  urldate = {2024-03-07},
  abstract = {OpenAI’s chatbot offers paraphrases, whereas Google offers quotes. Which do we prefer?},
  langid = {american},
  keywords = {algorithms,artificial intelligence (a.i.),chatgpt,images,internet,technology,writing},
  file = {/Users/maehr0000/Zotero/storage/4EEI9ZIA/chatgpt-is-a-blurry-jpeg-of-the-web.html}
}

@book{dignazio2023,
  title = {Data Feminism},
  author = {D'Ignazio, Catherine and Klein, Lauren F.},
  date = {2023},
  edition = {First MIT Press paperback edition},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  url = {https://data-feminism.mitpress.mit.edu/},
  abstract = {Today, data science is a form of power. It has been used to expose injustice, improve health outcomes, and topple governments. But it has also been used to discriminate, police, and surveil. This potential for good, on the one hand, and harm, on the other, makes it essential to ask: Data science by whom? Data science for whom? Data science with whose interests in mind? The narratives around big data and data science are overwhelmingly white, male, and techno-heroic. The authors present a new way of thinking about data science and data ethics--one that is informed by intersectional feminist thought.--back cover},
  isbn = {978-0-262-54718-5},
  langid = {english},
  annotation = {OCLC: 1354647893}
}

@book{karaganis2018,
  title = {Shadow {{Libraries}}: {{Access}} to {{Knowledge}} in {{Global Higher Education}}},
  shorttitle = {Shadow {{Libraries}}},
  editor = {Karaganis, Joe},
  date = {2018-05-04},
  series = {International {{Development Research Centre}}},
  publisher = {The MIT Press},
  location = {Cambridge},
  doi = {10.7551/mitpress/11339.001.0001},
  url = {https://direct.mit.edu/books/oa-edited-volume/3600/Shadow-LibrariesAccess-to-Knowledge-in-Global},
  urldate = {2024-11-25},
  abstract = {How students get the materials they need as opportunities for higher education expand but funding shrinks.From the top down, Shadow Libraries explores the},
  isbn = {978-0-262-34569-9 978-0-262-53501-4},
  langid = {english},
  file = {/Users/maehr0000/Zotero/storage/DNIUBVKK/Shadow-LibrariesAccess-to-Knowledge-in-Global.html}
}

@online{lacoste2019,
  title = {Quantifying the {{Carbon Emissions}} of {{Machine Learning}}},
  author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
  date = {2019-11-04},
  eprint = {1910.09700},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1910.09700},
  url = {http://arxiv.org/abs/1910.09700},
  urldate = {2024-11-25},
  abstract = {From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/maehr0000/Zotero/storage/SNHZS5VN/Lacoste et al. - 2019 - Quantifying the Carbon Emissions of Machine Learning.pdf;/Users/maehr0000/Zotero/storage/DDTIW7UU/1910.html}
}

@inproceedings{long2020,
  title = {What Is {{AI Literacy}}? {{Competencies}} and {{Design Considerations}}},
  shorttitle = {What Is {{AI Literacy}}?},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Long, Duri and Magerko, Brian},
  date = {2020-04-23},
  series = {{{CHI}} '20},
  pages = {1--16},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3313831.3376727},
  url = {https://doi.org/10.1145/3313831.3376727},
  urldate = {2024-11-25},
  abstract = {Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learner-centered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper's contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.},
  isbn = {978-1-4503-6708-0}
}

@book{loukissas2019,
  title = {All Data Are Local: Thinking Critically in a Data-Driven Society},
  shorttitle = {All Data Are Local},
  author = {Loukissas, Yanni A.},
  date = {2019},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  doi = {10.7551/mitpress/11543.001.0001},
  isbn = {978-0-262-03966-6 978-0-262-54517-4},
  pagetotal = {245},
  keywords = {Electronic information resource literacy,Media literacy}
}

@online{luccioni2023,
  title = {Stable {{Bias}}: {{Analyzing Societal Representations}} in {{Diffusion Models}}},
  shorttitle = {Stable {{Bias}}},
  author = {Luccioni, Alexandra Sasha and Akiki, Christopher and Mitchell, Margaret and Jernite, Yacine},
  date = {2023-11-09},
  eprint = {2303.11408},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2303.11408},
  url = {http://arxiv.org/abs/2303.11408},
  urldate = {2024-11-25},
  abstract = {As machine learning-enabled Text-to-Image (TTI) systems are becoming increasingly prevalent and seeing growing adoption as commercial services, characterizing the social biases they exhibit is a necessary first step to lowering their risk of discriminatory outcomes. This evaluation, however, is made more difficult by the synthetic nature of these systems' outputs: common definitions of diversity are grounded in social categories of people living in the world, whereas the artificial depictions of fictive humans created by these systems have no inherent gender or ethnicity. To address this need, we propose a new method for exploring the social biases in TTI systems. Our approach relies on characterizing the variation in generated images triggered by enumerating gender and ethnicity markers in the prompts, and comparing it to the variation engendered by spanning different professions. This allows us to (1) identify specific bias trends, (2) provide targeted scores to directly compare models in terms of diversity and representation, and (3) jointly model interdependent social variables to support a multidimensional analysis. We leverage this method to analyze images generated by 3 popular TTI systems (Dall-E 2, Stable Diffusion v 1.4 and 2) and find that while all of their outputs show correlations with US labor demographics, they also consistently under-represent marginalized identities to different extents. We also release the datasets and low-code interactive bias exploration platforms developed for this work, as well as the necessary tools to similarly evaluate additional TTI systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/maehr0000/Zotero/storage/B3EAT88L/Luccioni et al. - 2023 - Stable Bias Analyzing Societal Representations in Diffusion Models.pdf;/Users/maehr0000/Zotero/storage/WA9CUUS8/2303.html}
}

@online{mattu,
  title = {Machine {{Bias}}},
  author = {Mattu, Jeff Larson,Lauren Kirchner,Surya, Julia Angwin},
  url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
  urldate = {2024-11-25},
  abstract = {There’s software used across the country to predict future criminals. And it’s biased against blacks.},
  langid = {english},
  organization = {ProPublica},
  file = {/Users/maehr0000/Zotero/storage/SI4MXH3T/machine-bias-risk-assessments-in-criminal-sentencing.html}
}

@article{mehrabi2021,
  title = {A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  date = {2021-07-13},
  journaltitle = {ACM Comput. Surv.},
  volume = {54},
  number = {6},
  pages = {115:1--115:35},
  issn = {0360-0300},
  doi = {10.1145/3457607},
  url = {https://doi.org/10.1145/3457607},
  urldate = {2024-11-25},
  abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  file = {/Users/maehr0000/Zotero/storage/QAV39M9N/Mehrabi et al. - 2021 - A Survey on Bias and Fairness in Machine Learning.pdf}
}

@article{mueller2025,
  title = {It's Just Distributed Computing: {{Rethinking AI}} Governance},
  shorttitle = {It's Just Distributed Computing},
  author = {Mueller, Milton L.},
  date = {2025-02},
  journaltitle = {Telecommunications Policy},
  shortjournal = {Telecommunications Policy},
  pages = {102917},
  issn = {03085961},
  doi = {10.1016/j.telpol.2025.102917},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S030859612500014X},
  urldate = {2025-02-20},
  langid = {english}
}

@book{noble2018,
  title = {Algorithms of Oppression: How Search Engines Reinforce Racism},
  shorttitle = {Algorithms of Oppression},
  author = {Noble, Safiya Umoja},
  date = {2018},
  publisher = {New York university press},
  location = {New York},
  abstract = {"In Algorithms of Oppression, Safiya Umoja Noble challenges the idea that search engines like Google offer an equal playing field for all forms of ideas, identities, and activities. Data discrimination is a real social problem. Noble argues that the combination of private interests in promoting certain sites, along with the monopoly status of a relatively small number of Internet search engines, leads to a biased set of search algorithms that privilege whiteness and discriminate against people of color, especially women of color. Through an analysis of textual and media searches as well as extensive research on paid online advertising, Noble exposes a culture of racism and sexism in the way discoverability is created online. As search engines and their related companies grow in importance--operating as a source for email, a major vehicle for primary and secondary school learning, and beyond--understanding and reversing these disquieting trends and discriminatory practices are of utmost importance."--Page 4 of cover},
  isbn = {978-1-4798-3724-3 978-1-4798-4994-9},
  langid = {english}
}

@online{offert2024,
  title = {The {{Method}} of {{Critical AI Studies}}, {{A Propaedeutic}}},
  author = {Offert, Fabian and Dhaliwal, Ranjodh Singh},
  date = {2024-12-10},
  eprint = {2411.18833},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.18833},
  url = {http://arxiv.org/abs/2411.18833},
  urldate = {2025-02-20},
  abstract = {We outline some common methodological issues in the field of critical AI studies, including a tendency to overestimate the explanatory power of individual samples (the benchmark casuistry), a dependency on theoretical frameworks derived from earlier conceptualizations of computation (the black box casuistry), and a preoccupation with a cause-and-effect model of algorithmic harm (the stack casuistry). In the face of these issues, we call for, and point towards, a future set of methodologies that might take into account existing strengths in the humanistic close analysis of cultural objects.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/maehr0000/Zotero/storage/EJLVREDS/Offert and Dhaliwal - 2024 - The Method of Critical AI Studies, A Propaedeutic.pdf;/Users/maehr0000/Zotero/storage/XFCC4EVX/2411.html}
}

@book{oneil2016,
  title = {Weapons of {{Math Destruction}}: {{How Big Data Increases Inequality}} and {{Threatens Democracy}}},
  shorttitle = {Weapons of {{Math Destruction}}},
  author = {O'Neil, Cathy},
  date = {2016-08},
  edition = {First edition},
  publisher = {Crown Publishing Group},
  location = {New York},
  isbn = {978-0-553-41881-1 978-0-553-41883-5},
  pagetotal = {272},
  keywords = {21st century,Big data,Democracy,Mathematical models Moral and ethical aspects,Political aspects,Social aspects,Social conditions,Social indicators,United States},
  file = {/Users/maehr0000/Zotero/storage/6D8RXYFZ/Cathy O’Neil - Weapons of Math Destruction-Crown Publishers (2016).pdf;/Users/maehr0000/Zotero/storage/76UAYB9Z/Cathy O’Neil - Weapons of Math Destruction_ How Big Data Increases Inequality and Threatens Democracy (2016).epub}
}

@online{zotero-22193,
  title = {{{EU Artificial Intelligence Act}} | {{Up-to-date}} Developments and Analyses of the {{EU AI Act}}},
  url = {https://artificialintelligenceact.eu/},
  urldate = {2024-11-25},
  langid = {american}
}

@online{zotero-22194,
  title = {AlgorithmWatch},
  url = {https://algorithmwatch.org/de/},
  urldate = {2024-11-25},
  abstract = {AlgorithmWatch ist eine gemeinnützige Organisation mit dem Ziel, Prozesse algorithmischer Entscheidungsfindung zu betrachten und einzuordnen, die eine gesellschaftliche Relevanz haben – die also entweder menschliche Entscheidungen vorhersagen oder vorbestimmen, oder Entscheidungen automatisiert treffen.},
  langid = {ngerman},
  organization = {AlgorithmWatch}
}
