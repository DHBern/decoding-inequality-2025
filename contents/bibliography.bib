@book{2017c,
  title = {The {{Age}} of {{Surveillance Capitalism}}: {{The Fight}} for a {{Human Future}} at the {{New Frontier}} of {{Power}}},
  shorttitle = {The Age of Surveillance Capitalism},
  author = {Zuboff, Shoshana},
  date = {2017-06-27T21:33:47+00:00},
  edition = {First trade paperback edition},
  publisher = {PublicAffairs},
  location = {New York, NY},
  url = {https://www.hachettebookgroup.com/titles/shoshana-zuboff/the-age-of-surveillance-capitalism/9781610395694/?lens=publicaffairs},
  abstract = {"Shoshana Zuboff, named "the true prophet of the information age" by the Financial Times, has always been ahead of her time. Her seminal book In the Age of the Smart Machine foresaw the consequences of a then-unfolding era of computer technology. Now, three decades later she asks why the once-celebrated miracle of digital is turning into a nightmare. Zuboff tackles the social, political, business, personal, and technological meaning of "surveillance capitalism" as an unprecedented new market form. It is not simply about tracking us and selling ads, it is the business model for an ominous new marketplace that aims at nothing less than predicting and modifying our everyday behavior--where we go, what we do, what we say, how we feel, who we're with. The consequences of surveillance capitalism for us as individuals and as a society vividly come to life in The Age of Surveillance Capitalism's pathbreaking analysis of power. The threat has shifted from a totalitarian "big brother" state to a universal global architecture of automatic sensors and smart capabilities: A "big other" that imposes a fundamentally new form of power and unprecedented concentrations of knowledge in private companies--free from democratic oversight and control"--},
  isbn = {978-1-61039-569-4 978-1-5417-5800-1},
  langid = {american},
  pagetotal = {691},
  file = {/Users/maehr0000/Zotero/storage/TSUS5UZB/Zuboff - 2020 - The age of surveillance capitalism the fight for a human future at the new frontier of power.pdf;/Users/maehr0000/Zotero/storage/S2NAG237/9781610395694.html}
}

@online{2018e,
  title = {Jobs fÃ¼r FlÃ¼chtlinge - Algorithmus verteilt neu Asylbewerber auf Kantone},
  date = {2018-05-10T08:57:00+02:00},
  url = {https://www.srf.ch/news/schweiz/jobs-fuer-fluechtlinge-algorithmus-verteilt-neu-asylbewerber-auf-kantone},
  urldate = {2024-11-25},
  abstract = {FÃ¼r die Â«Big DataÂ» des Migrationsamtes interessieren sich weltweit auch andere Staaten.},
  langid = {ngerman},
  organization = {Schweizer Radio und Fernsehen (SRF)},
  file = {/Users/maehr0000/Zotero/storage/77I7JNJQ/jobs-fuer-fluechtlinge-algorithmus-verteilt-neu-asylbewerber-auf-kantone.html}
}

@online{2018f,
  title = {Algorithmus verbessert Erwerbschancen von FlÃ¼chtlingen},
  date = {2018-01-18},
  url = {https://ethz.ch/de/news-und-veranstaltungen/eth-news/news/2018/01/algorithmus-verbessert-erwerbschancen-von-fluechtlingen.html},
  urldate = {2024-11-25},
  abstract = {Die ErwerbstÃ¤tigkeit von Asylsuchenden in der Schweiz kÃ¶nnte mit einem datengestÃ¼tzten Ansatz von 15 auf 26 Prozent erhÃ¶ht werden. Zu diesem Ergebnis kommen Sozialwissenschaftler und Sozialwissenschaftlerinnen aus der Schweiz und den USA mit Beteiligung der ETH-Professur fÃ¼r Politikanalyse in der Wissenschaftszeitschrift Â«ScienceÂ».},
  langid = {ngerman},
  organization = {ETH ZÃ¼rich},
  file = {/Users/maehr0000/Zotero/storage/VQFSHXH6/algorithmus-verbessert-erwerbschancen-von-fluechtlingen.html}
}

@inreference{2022j,
  title = {\emph{Algorithms of }{{\emph{Oppression}}}},
  booktitle = {Wikipedia},
  date = {2022-06-28T00:04:29Z},
  url = {https://en.wikipedia.org/w/index.php?title=Algorithms_of_Oppression&oldid=1095369660},
  urldate = {2024-11-23},
  abstract = {Algorithms of Oppression: How Search Engines Reinforce Racism is a 2018 book by Safiya Umoja Noble in the fields of information science, machine learning, and human-computer interaction.},
  langid = {english},
  annotation = {Page Version ID: 1095369660},
  file = {/Users/maehr0000/Zotero/storage/NSZMZ7HF/Algorithms_of_Oppression.html}
}

@inreference{2024c,
  title = {Surveillance Capitalism},
  booktitle = {Wikipedia},
  date = {2024-08-12T09:16:35Z},
  url = {https://en.wikipedia.org/w/index.php?title=Surveillance_capitalism&oldid=1239902991},
  urldate = {2024-11-21},
  abstract = {Surveillance capitalism is a concept in political economics which denotes the widespread collection and commodification of personal data by corporations. This phenomenon is distinct from government surveillance, although the two can be mutually reinforcing. The concept of surveillance capitalism, as described by Shoshana Zuboff, is driven by a profit-making incentive, and arose as advertising companies, led by Google's AdWords, saw the possibilities of using personal data to target consumers more precisely. Increased data collection may have various benefits for individuals and society, such as self-optimization (the quantified self), societal optimizations (e.g., by smart cities) and optimized services (including various web applications). However, as capitalism focuses on expanding the proportion of social life that is open to data collection and data processing, this can have significant implications for vulnerability and control of society, as well as for privacy. The economic pressures of capitalism are driving the intensification of online connection and monitoring, with spaces of social life opening up to saturation by corporate actors, directed at making profits and/or regulating behavior. Therefore, personal data points increased in value after the possibilities of targeted advertising were known. As a result, the increasing price of data has limited access to the purchase of personal data points to the richest in society.},
  langid = {english},
  annotation = {Page Version ID: 1239902991}
}

@inreference{2024d,
  title = {Attention {{Is All You Need}}},
  booktitle = {Wikipedia},
  date = {2024-11-21T18:38:06Z},
  url = {https://en.wikipedia.org/w/index.php?title=Attention_Is_All_You_Need&oldid=1258803471},
  urldate = {2024-11-23},
  abstract = {"Attention Is All You Need" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique's potential for other tasks like question answering and what is now known as multimodal Generative AI. The paper's title is a reference to the song "All You Need Is Love" by the Beatles. The name "Transformer" was picked because Uszkoreit liked the sound of that word. An early design document was titled "Transformers: Iterative Self-Attention and Processing for Various Tasks", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer. Some early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on "The Transformer", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation. As of 2024, the paper has been cited more than 100,000 times.},
  langid = {english},
  annotation = {Page Version ID: 1258803471},
  file = {/Users/maehr0000/Zotero/storage/WZZS8KCH/Attention_Is_All_You_Need.html}
}

@article{bansak2018,
  title = {Improving Refugee Integration through Data-Driven Algorithmic Assignment},
  author = {Bansak, Kirk and Ferwerda, Jeremy and Hainmueller, Jens and Dillon, Andrea and Hangartner, Dominik and Lawrence, Duncan and Weinstein, Jeremy},
  date = {2018-01-19},
  journaltitle = {Science},
  volume = {359},
  number = {6373},
  pages = {325--329},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aao4408},
  url = {https://www.science.org/doi/10.1126/science.aao4408},
  urldate = {2024-11-25},
  abstract = {Developed democracies are settling an increased number of refugees, many of whom face challenges integrating into host societies. We developed a flexible data-driven algorithm that assigns refugees across resettlement locations to improve integration outcomes. The algorithm uses a combination of supervised machine learning and optimal matching to discover and leverage synergies between refugee characteristics and resettlement sites. The algorithm was tested on historical registry data from two countries with different assignment regimes and refugee populations, the United States and Switzerland. Our approach led to gains of roughly 40 to 70\%, on average, in refugeesâ€™ employment outcomes relative to current assignment practices. This approach can provide governments with a practical and cost-efficient policy tool that can be immediately implemented within existing institutional structures.}
}

@inproceedings{bender2021,
  title = {On the {{Dangers}} of {{Stochastic Parrots}}: {{Can Language Models Be Too Big}}? ðŸ¦œ},
  shorttitle = {On the {{Dangers}} of {{Stochastic Parrots}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  date = {2021-03-01},
  series = {{{FAccT}} '21},
  pages = {610--623},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3442188.3445922},
  url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
  urldate = {2024-11-24},
  abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  isbn = {978-1-4503-8309-7},
  file = {/Users/maehr0000/Zotero/storage/I67QAH43/Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language Models Be Too Big .pdf}
}

@inproceedings{buolamwini2018,
  title = {Gender {{Shades}}: {{Intersectional Accuracy Disparities}} in {{Commercial Gender Classification}}},
  shorttitle = {Gender {{Shades}}},
  booktitle = {Proceedings of the 1st {{Conference}} on {{Fairness}}, {{Accountability}} and {{Transparency}}},
  author = {Buolamwini, Joy and Gebru, Timnit},
  date = {2018-01-21},
  pages = {77--91},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
  urldate = {2024-11-25},
  abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
  eventtitle = {Conference on {{Fairness}}, {{Accountability}} and {{Transparency}}},
  langid = {english},
  file = {/Users/maehr0000/Zotero/storage/9VEVQA9X/Buolamwini and Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities in Commercial Gender Classification.pdf;/Users/maehr0000/Zotero/storage/F3RQCG3H/Buolamwini and Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities in Commercial Gender Classification.pdf}
}

@book{buolamwini2023,
  title = {Unmasking {{AI}}: A Story of Hope and Justice in a World of Machines},
  shorttitle = {Unmasking {{AI}}},
  author = {Buolamwini, Joy},
  date = {2023},
  edition = {First edition},
  publisher = {Random House},
  location = {New York},
  abstract = {"Dr. Joy Buolamwini is the self-described "Poet of Code" who has had a lifelong passion for computer science, engineering, and art-disciplines that, she felt, pushed the boundaries of reality. After tinkering with robotics as a high school student in Tennessee, to developing mobile apps in Zambia as a Fulbright fellow, Buolamwini eventually found herself at MIT. As a graduate student at the "Future Factory," Buolamwini's groundbreaking research revealed that AI systems-from leading tech companies-were consistently failing on non-male, non-white bodies. In Unmasking AI, Buolamwini goes beyond the news headlines about racism, colorism, and sexism in Big Tech to tell the remarkable story of how she uncovered what she calls "the coded gaze"-evidence of racial and gender bias in tech-and galvanized the movement to prevent AI harms by founding the Algorithmic Justice League. Applying an intersectional lens to both tech industry and research sector, Buolamwini shows how race, gender, and ability bias can overlap and render broad swaths of humanity vulnerable in our AI-dependent world. Computers, she reminds us, are reflections of both the aspirations and the limitations of the people who create them"--},
  isbn = {978-0-593-24185-1},
  pagetotal = {1},
  keywords = {Artificial intelligence,Discrimination in science,Moral and ethical aspects,Philosophy,Sex discrimination in science,Social aspects}
}

@article{chen2023,
  title = {Ethics and Discrimination in Artificial Intelligence-Enabled Recruitment Practices},
  author = {Chen, Zhisheng},
  date = {2023-09-13},
  journaltitle = {Humanities and Social Sciences Communications},
  shortjournal = {Humanit Soc Sci Commun},
  volume = {10},
  number = {1},
  pages = {1--12},
  publisher = {Palgrave},
  issn = {2662-9992},
  doi = {10.1057/s41599-023-02079-x},
  url = {https://www.nature.com/articles/s41599-023-02079-x},
  urldate = {2024-11-25},
  abstract = {This study aims to address the research gap on algorithmic discrimination caused by AI-enabled recruitment and explore technical and managerial solutions. The primary research approach used is a literature review. The findings suggest that AI-enabled recruitment has the potential to enhance recruitment quality, increase efficiency, and reduce transactional work. However, algorithmic bias results in discriminatory hiring practices based on gender, race, color, and personality traits. The study indicates that algorithmic bias stems from limited raw data sets and biased algorithm designers. To mitigate this issue, it is recommended to implement technical measures, such as unbiased dataset frameworks and improved algorithmic transparency, as well as management measures like internal corporate ethical governance and external oversight. Employing Grounded Theory, the study conducted survey analysis to collect firsthand data on respondentsâ€™ experiences and perceptions of AI-driven recruitment applications and discrimination.},
  langid = {english},
  keywords = {Business and management,Science,technology and society},
  file = {/Users/maehr0000/Zotero/storage/4ATMGF7N/Chen - 2023 - Ethics and discrimination in artificial intelligence-enabled recruitment practices.pdf}
}

@article{chiang2023a,
  entrysubtype = {magazine},
  title = {{{ChatGPT Is}} a {{Blurry JPEG}} of the {{Web}}},
  author = {Chiang, Ted},
  date = {2023-02-09},
  journaltitle = {The New Yorker},
  issn = {0028-792X},
  url = {https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web},
  urldate = {2024-11-25},
  abstract = {OpenAIâ€™s chatbot offers paraphrases, whereas Google offers quotes. Which do we prefer?},
  langid = {american},
  keywords = {algorithms,artificial intelligence (a.i.),chatgpt,images,internet,technology,writing},
  file = {/Users/maehr0000/Zotero/storage/PT9SPMW4/chatgpt-is-a-blurry-jpeg-of-the-web.html}
}

@book{dignazio2023,
  title = {Data Feminism},
  author = {D'Ignazio, Catherine and Klein, Lauren F.},
  date = {2023},
  edition = {First MIT Press paperback edition},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  url = {https://data-feminism.mitpress.mit.edu/},
  abstract = {Today, data science is a form of power. It has been used to expose injustice, improve health outcomes, and topple governments. But it has also been used to discriminate, police, and surveil. This potential for good, on the one hand, and harm, on the other, makes it essential to ask: Data science by whom? Data science for whom? Data science with whose interests in mind? The narratives around big data and data science are overwhelmingly white, male, and techno-heroic. The authors present a new way of thinking about data science and data ethics--one that is informed by intersectional feminist thought.--back cover},
  isbn = {978-0-262-54718-5},
  langid = {english},
  annotation = {OCLC: 1354647893}
}

@book{karaganis2018,
  title = {Shadow {{Libraries}}: {{Access}} to {{Knowledge}} in {{Global Higher Education}}},
  shorttitle = {Shadow {{Libraries}}},
  editor = {Karaganis, Joe},
  date = {2018-05-04},
  series = {International {{Development Research Centre}}},
  publisher = {The MIT Press},
  location = {Cambridge},
  doi = {10.7551/mitpress/11339.001.0001},
  url = {https://direct.mit.edu/books/oa-edited-volume/3600/Shadow-LibrariesAccess-to-Knowledge-in-Global},
  urldate = {2024-11-25},
  abstract = {How students get the materials they need as opportunities for higher education expand but funding shrinks.From the top down, Shadow Libraries explores the},
  isbn = {978-0-262-34569-9 978-0-262-53501-4},
  langid = {english},
  file = {/Users/maehr0000/Zotero/storage/DNIUBVKK/Shadow-LibrariesAccess-to-Knowledge-in-Global.html}
}

@online{lacoste2019,
  title = {Quantifying the {{Carbon Emissions}} of {{Machine Learning}}},
  author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
  date = {2019-11-04},
  eprint = {1910.09700},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1910.09700},
  url = {http://arxiv.org/abs/1910.09700},
  urldate = {2024-11-25},
  abstract = {From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/maehr0000/Zotero/storage/SNHZS5VN/Lacoste et al. - 2019 - Quantifying the Carbon Emissions of Machine Learning.pdf;/Users/maehr0000/Zotero/storage/DDTIW7UU/1910.html}
}

@inproceedings{long2020,
  title = {What Is {{AI Literacy}}? {{Competencies}} and {{Design Considerations}}},
  shorttitle = {What Is {{AI Literacy}}?},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Long, Duri and Magerko, Brian},
  date = {2020-04-23},
  series = {{{CHI}} '20},
  pages = {1--16},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3313831.3376727},
  url = {https://doi.org/10.1145/3313831.3376727},
  urldate = {2024-11-25},
  abstract = {Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learner-centered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper's contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.},
  isbn = {978-1-4503-6708-0}
}

@book{loukissas2019,
  title = {All Data Are Local: Thinking Critically in a Data-Driven Society},
  shorttitle = {All Data Are Local},
  author = {Loukissas, Yanni A.},
  date = {2019},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  isbn = {978-0-262-03966-6 978-0-262-54517-4},
  pagetotal = {245},
  keywords = {Electronic information resource literacy,Media literacy}
}

@online{luccioni2023,
  title = {Stable {{Bias}}: {{Analyzing Societal Representations}} in {{Diffusion Models}}},
  shorttitle = {Stable {{Bias}}},
  author = {Luccioni, Alexandra Sasha and Akiki, Christopher and Mitchell, Margaret and Jernite, Yacine},
  date = {2023-11-09},
  eprint = {2303.11408},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2303.11408},
  url = {http://arxiv.org/abs/2303.11408},
  urldate = {2024-11-25},
  abstract = {As machine learning-enabled Text-to-Image (TTI) systems are becoming increasingly prevalent and seeing growing adoption as commercial services, characterizing the social biases they exhibit is a necessary first step to lowering their risk of discriminatory outcomes. This evaluation, however, is made more difficult by the synthetic nature of these systems' outputs: common definitions of diversity are grounded in social categories of people living in the world, whereas the artificial depictions of fictive humans created by these systems have no inherent gender or ethnicity. To address this need, we propose a new method for exploring the social biases in TTI systems. Our approach relies on characterizing the variation in generated images triggered by enumerating gender and ethnicity markers in the prompts, and comparing it to the variation engendered by spanning different professions. This allows us to (1) identify specific bias trends, (2) provide targeted scores to directly compare models in terms of diversity and representation, and (3) jointly model interdependent social variables to support a multidimensional analysis. We leverage this method to analyze images generated by 3 popular TTI systems (Dall-E 2, Stable Diffusion v 1.4 and 2) and find that while all of their outputs show correlations with US labor demographics, they also consistently under-represent marginalized identities to different extents. We also release the datasets and low-code interactive bias exploration platforms developed for this work, as well as the necessary tools to similarly evaluate additional TTI systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/maehr0000/Zotero/storage/B3EAT88L/Luccioni et al. - 2023 - Stable Bias Analyzing Societal Representations in Diffusion Models.pdf;/Users/maehr0000/Zotero/storage/WA9CUUS8/2303.html}
}

@online{mattu,
  title = {Machine {{Bias}}},
  author = {Mattu, Jeff Larson,Lauren Kirchner,Surya, Julia Angwin},
  url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
  urldate = {2024-11-25},
  abstract = {Thereâ€™s software used across the country to predict future criminals. And itâ€™s biased against blacks.},
  langid = {english},
  organization = {ProPublica},
  file = {/Users/maehr0000/Zotero/storage/SI4MXH3T/machine-bias-risk-assessments-in-criminal-sentencing.html}
}

@article{mehrabi2021,
  title = {A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  date = {2021-07-13},
  journaltitle = {ACM Comput. Surv.},
  volume = {54},
  number = {6},
  pages = {115:1--115:35},
  issn = {0360-0300},
  doi = {10.1145/3457607},
  url = {https://doi.org/10.1145/3457607},
  urldate = {2024-11-25},
  abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  file = {/Users/maehr0000/Zotero/storage/QAV39M9N/Mehrabi et al. - 2021 - A Survey on Bias and Fairness in Machine Learning.pdf}
}

@book{noble2018,
  title = {Algorithms of Oppression: How Search Engines Reinforce Racism},
  shorttitle = {Algorithms of Oppression},
  author = {Noble, Safiya Umoja},
  date = {2018},
  publisher = {New York university press},
  location = {New York},
  abstract = {"In Algorithms of Oppression, Safiya Umoja Noble challenges the idea that search engines like Google offer an equal playing field for all forms of ideas, identities, and activities. Data discrimination is a real social problem. Noble argues that the combination of private interests in promoting certain sites, along with the monopoly status of a relatively small number of Internet search engines, leads to a biased set of search algorithms that privilege whiteness and discriminate against people of color, especially women of color. Through an analysis of textual and media searches as well as extensive research on paid online advertising, Noble exposes a culture of racism and sexism in the way discoverability is created online. As search engines and their related companies grow in importance--operating as a source for email, a major vehicle for primary and secondary school learning, and beyond--understanding and reversing these disquieting trends and discriminatory practices are of utmost importance."--Page 4 of cover},
  isbn = {978-1-4798-3724-3 978-1-4798-4994-9},
  langid = {english}
}

@book{oneil2016,
  title = {Weapons of {{Math Destruction}}: {{How Big Data Increases Inequality}} and {{Threatens Democracy}}},
  shorttitle = {Weapons of {{Math Destruction}}},
  author = {O'Neil, Cathy},
  date = {2016-08},
  edition = {First edition},
  publisher = {Crown Publishing Group},
  location = {New York},
  isbn = {978-0-553-41881-1 978-0-553-41883-5},
  pagetotal = {272},
  keywords = {21st century,Big data,Democracy,Mathematical models Moral and ethical aspects,Political aspects,Social aspects,Social conditions,Social indicators,United States}
}

@online{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2024-11-23},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/maehr0000/Zotero/storage/J43WPMMU/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/Users/maehr0000/Zotero/storage/ZLLVG9MY/1706.html}
}

@online{zotero-22193,
  title = {{{EU Artificial Intelligence Act}} | {{Up-to-date}} Developments and Analyses of the {{EU AI Act}}},
  url = {https://artificialintelligenceact.eu/},
  urldate = {2024-11-25},
  langid = {american}
}

@online{zotero-22194,
  title = {AlgorithmWatch},
  url = {https://algorithmwatch.org/de/},
  urldate = {2024-11-25},
  abstract = {AlgorithmWatch ist eine gemeinnÃ¼tzige Organisation mit dem Ziel, Prozesse algorithmischer Entscheidungsfindung zu betrachten und einzuordnen, die eine gesellschaftliche Relevanz haben â€“ die also entweder menschliche Entscheidungen vorhersagen oder vorbestimmen, oder Entscheidungen automatisiert treffen.},
  langid = {ngerman},
  organization = {AlgorithmWatch}
}
